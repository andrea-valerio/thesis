{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da     #scalable parallel computing\n",
    "import json\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. File Aggregation\n",
    " Reduction of the number of the .csv files by aggregating them parameter-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set root directory\n",
    "root_dir = './exports'\n",
    "os.makedirs(root_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_columns(ddf):   # Set 'participant_id' as first column\n",
    "    col_order = ['participant_id'] + [col for col in ddf.columns if col != 'participant_id']\n",
    "    return ddf[col_order]\n",
    "\n",
    "def aggregate_files(root_dir, output_dir, file_name, sub = False):  # Aggregate csvs into one ddf and save the file\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    all_ddfs = []\n",
    "    for participant_folder in sorted(os.listdir(root_dir)):\n",
    "        participant_path = os.path.join(root_dir, participant_folder)\n",
    "        if not os.path.isdir(participant_path) or participant_folder == 'P03':\n",
    "            continue  # skip if non-directories or excluded participant\n",
    "        \n",
    "        file_path = os.path.join(participant_path, 'surfaces', file_name) if sub else os.path.join(participant_path, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            if file_name == 'surface_gaze_distribution.csv' or file_name == 'surface_visibility.csv':   # filter out the first-row (total count)\n",
    "                df = pd.read_csv(file_path, header=None)\n",
    "                col_names = df.iloc[1].tolist() # get column names\n",
    "                df = df.iloc[2:]                # get values only\n",
    "                df.columns = col_names          # set column names\n",
    "                ddf = dd.from_pandas(df, npartitions=1) # convert to dd\n",
    "            else:\n",
    "                ddf = dd.read_csv(file_path)            # read as dd\n",
    "            ddf['participant_id'] = int(participant_folder[1:]) # add 'participant-id' column\n",
    "            ddf = reorder_columns(ddf)                          # set 'participant_id' as first column\n",
    "            all_ddfs.append(ddf)    # add to the aggregation list\n",
    "        else:\n",
    "            print(f\"File {file_name} not found in {participant_folder}\")\n",
    "    \n",
    "    if all_ddfs:\n",
    "        aggregated_ddf = dd.concat(all_ddfs)    # aggregate the list into one ddf\n",
    "        aggregated_ddf.to_csv(os.path.join(output_dir, f'all_{file_name}'), single_file=True)   # save ddf to file\n",
    "    else:\n",
    "        print(f\"No data found for {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests / Single files\n",
    "# aggregate_files(root_dir, './aggregated_data', 'gaze_positions.csv') # main folder\n",
    "# aggregate_files(root_dir, './aggregated_data/all_surfaces', 'fixations_on_surface_HiDrive_Studie2.csv', sub = True) # subfolder\n",
    "# aggregate_files(root_dir, './aggregated_data/all_surfaces', 'surface_visibility.csv', sub = True) # exception1 in subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of files to aggregate\n",
    "files_to_aggregate = [\n",
    "    'blink_detection_report.csv',\n",
    "    'blinks.csv',\n",
    "    'export_info.csv',\n",
    "    'fixation_report.csv',\n",
    "    'fixations.csv',\n",
    "    'gaze_positions.csv',\n",
    "    'pupil_positions.csv',\n",
    "    'world_timestamps.csv',\n",
    "]\n",
    "\n",
    "files_to_aggregate_sub = [\n",
    "    'fixations_on_surface_HiDrive_Studie2.csv',\n",
    "    'gaze_positions_on_surface_HiDrive_Studie2.csv',\n",
    "    'marker_detections.csv',\n",
    "    'surf_positions_HiDrive_Studie2.csv',\n",
    "    'surface_events.csv',\n",
    "    'surface_gaze_distribution.csv',\n",
    "    'surface_visibility.csv',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the aggregation\n",
    "for file in files_to_aggregate:\n",
    "    aggregate_files(root_dir, './aggregated_data', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_sub in files_to_aggregate_sub:\n",
    "    aggregate_files(root_dir, './aggregated_data/all_surfaces', file_sub, sub = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Access to other types of file or info\n",
    "Functions definition for retrieving non-aggregatable files or info (.mp4, .png, surf_gaze, surf_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file path for videos and images\n",
    "def get_path(participant_folder, file_type):\n",
    "    participant_path = os.path.join(root_dir, participant_folder)\n",
    "\n",
    "    if not os.path.isdir(participant_path):\n",
    "        print(f\"Participant folder {participant_folder} does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    if file_type == 'mp4':\n",
    "        file_path = os.path.join(participant_path, 'world.mp4')\n",
    "    elif file_type == 'png':\n",
    "        file_path = os.path.join(participant_path, 'surfaces', 'heatmap_HiDrive_Studie2.png')\n",
    "    else:\n",
    "        print(\"Invalid file type. Use 'mp4' for video files or 'png' for image files.\")\n",
    "        return None\n",
    "    \n",
    "    if os.path.isfile(file_path):\n",
    "        return file_path\n",
    "    else:\n",
    "        print(f\"No '{file_type}' file found in {participant_folder}.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "#print(get_path('P01', 'mp4'))\n",
    "#print(get_path('P02', 'png'))\n",
    "#print(get_path('P14', 'png'))\n",
    "#print(get_path('P19', 'jpeg'))\n",
    "#print(get_path('P21', 'png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info on surface gaze or visibility count\n",
    "def get_info(participant_folder, info_type):\n",
    "    participant_path = os.path.join(root_dir, participant_folder)\n",
    "\n",
    "    if not os.path.isdir(participant_path):\n",
    "        print(f\"Participant folder {participant_folder} does not exist.\")\n",
    "        return None\n",
    "    \n",
    "    if info_type == 'gaz':\n",
    "        file_path = os.path.join(participant_path, 'surfaces', 'surface_gaze_distribution.csv')\n",
    "    elif info_type == 'vis':\n",
    "        file_path = os.path.join(participant_path, 'surfaces', 'surface_visibility.csv')\n",
    "    else:\n",
    "        print(\"Invalid file type. Use 'gaz' for gaze surface info or 'vis' for visibility surface info.\")\n",
    "        return None\n",
    "    \n",
    "    if os.path.isfile(file_path):\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "        info = df.iloc[0, 1]\n",
    "        return info\n",
    "    else:\n",
    "        print(f\"No '{info_type}' info found in {participant_folder}.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "#print(get_info('P01', 'gaz'))\n",
    "#print(get_info('P02', 'vis'))\n",
    "#print(get_info('P14', 'vis'))\n",
    "#print(get_info('P19', 'jpeg'))\n",
    "#print(get_info('P21', 'pup'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Categorical Variables\n",
    "Identify categorical variables and print out the levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate output variable and set root directory\n",
    "cat_vars = {}\n",
    "root_dir = './aggregated_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categoricals(df, cat_vars, file_name, incl_col, unique_threshold=0.1,):\n",
    "    total_rows = len(df)\n",
    "    for column in df.columns:\n",
    "        if column not in incl_col:\n",
    "            continue  # Skip columns that are not in the list\n",
    "        \n",
    "        unique_count = df[column].nunique()\n",
    "        unique_ratio = unique_count / total_rows\n",
    "        if unique_ratio < unique_threshold:\n",
    "            unique_values = df[column].unique().tolist()\n",
    "            if file_name not in cat_vars:\n",
    "                cat_vars[file_name] = {}\n",
    "            cat_vars[file_name][column] = unique_values\n",
    "    \n",
    "    return cat_vars\n",
    "\n",
    "def save_to_json(dictionary, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(dictionary, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to verify\n",
    "columns_to_include = [\n",
    "    'index',\n",
    "    'filter_response',\n",
    "    'base_data',\n",
    "    'value',\n",
    "    'method',\n",
    "    'fixation classifier',\n",
    "    'on_surf',\n",
    "    'surface_name',\n",
    "    'event_type',\n",
    "    'num_definition_markers',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_blinks.csv saved.\n",
      "all_blink_detection_report.csv saved.\n",
      "all_export_info.csv saved.\n",
      "all_fixations.csv saved.\n",
      "all_fixation_report.csv saved.\n",
      "all_gaze_positions.csv saved.\n",
      "all_pupil_positions.csv saved.\n",
      "all_world_timestamps.csv saved.\n",
      "all_fixations_on_surface_HiDrive_Studie2.csv saved.\n",
      "all_gaze_positions_on_surface_HiDrive_Studie2.csv saved.\n",
      "all_marker_detections.csv saved.\n",
      "all_surface_events.csv saved.\n",
      "all_surface_gaze_distribution.csv saved.\n",
      "all_surface_visibility.csv saved.\n",
      "all_surf_positions_HiDrive_Studie2.csv saved.\n",
      "Dictionary saved to categorical_columns.json\n"
     ]
    }
   ],
   "source": [
    "# Run the analysis\n",
    "for subdir, _, files in os.walk(root_dir): # iterate through the files\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            cat_vars = get_categoricals(df, cat_vars, file, columns_to_include)\n",
    "            print(f\"{file} saved.\")\n",
    "\n",
    "# Save the dictionary to a JSON file\n",
    "file_path = 'categorical_columns.json'\n",
    "save_to_json(cat_vars, file_path)\n",
    "print(f\"Dictionary saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Loading\n",
    "Load the data into one dictionary variable as ddfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ddfs into dictionary (exception for 'all_surf_pos', df)\n",
    "root_dir = './aggregated_data'\n",
    "def load_data(root_dir, exception=False):    # Get csv file reads into one dictionary\n",
    "    data = {}\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            file_name = os.path.splitext(file)[0]   #file name without '.csv'\n",
    "            if file.endswith('.csv') and file != 'all_surf_positions_HiDrive_Studie2.csv':\n",
    "                data[file_name] = dd.read_csv(file_path)    #read and attach to dict\n",
    "            elif exception and file == 'all_surf_positions_HiDrive_Studie2.csv':\n",
    "                data [file_name] = pd.read_csv(file_path, converters=converters)    #read as normal and attach to dict\n",
    "    return data\n",
    "def parse(filedata): # Manually read the column\n",
    "    output = []\n",
    "    for line in filedata.split('\\n'): # split into lines\n",
    "        line = line.strip().rstrip(']').lstrip('[') #remove whitespace and brackets\n",
    "        if not line:  \n",
    "            continue    #skip empty lines\n",
    "        line = line.split() #split into cell\n",
    "        row = []\n",
    "        for cell in line:\n",
    "            cell = cell.strip()     #remove whitespace\n",
    "            if not cell.strip():\n",
    "                continue    #skip empty cells\n",
    "            row.append(float(cell)) #convert to float and add\n",
    "        output.append(row)\n",
    "    return output\n",
    "converters = {\n",
    "    \"img_to_surf_trans\": parse,\n",
    "    \"surf_to_img_trans\": parse,\n",
    "    \"dist_img_to_surf_trans\": parse,\n",
    "    \"surf_to_dist_img_trans\": parse,\n",
    "}\n",
    "\n",
    "data = load_data(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Unix Timestamps\n",
    "Convert pupil timestamps into unix timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ddfs into dictionary (exception for 'all_surf_pos', df)\n",
    "root_dir = './aggregated_data'\n",
    "def load_data(root_dir, exception=True):    # Get csv file reads into one dictionary\n",
    "    data = {}\n",
    "    for subdir, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            file_name = os.path.splitext(file)[0]   #file name without '.csv'\n",
    "            if file.endswith('.csv') and file != 'all_surf_positions_HiDrive_Studie2.csv':\n",
    "                data[file_name] = dd.read_csv(file_path)    #read and attach to dict\n",
    "            elif exception and file == 'all_surf_positions_HiDrive_Studie2.csv':\n",
    "                data [file_name] = pd.read_csv(file_path, converters=converters)    #read as normal and attach to dict\n",
    "    return data\n",
    "def parse(filedata): # Manually read the column\n",
    "    output = []\n",
    "    for line in filedata.split('\\n'): # split into lines\n",
    "        line = line.strip().rstrip(']').lstrip('[') #remove whitespace and brackets\n",
    "        if not line:  \n",
    "            continue    #skip empty lines\n",
    "        line = line.split() #split into cell\n",
    "        row = []\n",
    "        for cell in line:\n",
    "            cell = cell.strip()     #remove whitespace\n",
    "            if not cell.strip():\n",
    "                continue    #skip empty cells\n",
    "            row.append(float(cell)) #convert to float and add\n",
    "        output.append(row)\n",
    "    return output\n",
    "converters = {\n",
    "    \"img_to_surf_trans\": parse,\n",
    "    \"surf_to_img_trans\": parse,\n",
    "    \"dist_img_to_surf_trans\": parse,\n",
    "    \"surf_to_dist_img_trans\": parse,\n",
    "}\n",
    "\n",
    "data = load_data(root_dir)\n",
    "participant_ids = [1,2,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conversion parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1710995923.0638132,\n",
       " 2: 1710995923.0869443,\n",
       " 4: 1710995922.0329595,\n",
       " 5: 1710995922.054,\n",
       " 6: 1710422800.0617552,\n",
       " 7: 1710422798.8454993,\n",
       " 8: 1710837270.471259,\n",
       " 9: 1710837270.4714973,\n",
       " 10: 1711015966.974017,\n",
       " 11: 1711015966.8716204,\n",
       " 12: 1711015966.3487737,\n",
       " 13: 1711015966.4835043,\n",
       " 14: 1711015966.4888632,\n",
       " 15: 1711377444.7161496,\n",
       " 16: 1711377443.4752297,\n",
       " 17: 1711527764.3984728,\n",
       " 18: 1711545871.2865818,\n",
       " 19: 1711545870.2516506,\n",
       " 20: 1711545870.261577}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unix offsets per participant\n",
    "json_names = {\n",
    "    1: 'p01',\n",
    "    2: 'p02',\n",
    "    3: '-',\n",
    "    4: 'p04',\n",
    "    5: 'p05',\n",
    "    6: 'p06',\n",
    "    7: 'p07',\n",
    "    8: 'p08',\n",
    "    9: 'p09',\n",
    "    10: 'p10',\n",
    "    11: 'p11',\n",
    "    12: 'p12',\n",
    "    13: 'p13',\n",
    "    14: 'p14',\n",
    "    15: 'p15',\n",
    "    16: 'p16',\n",
    "    17: 'p17',\n",
    "    18: 'p18',\n",
    "    19: 'p19',\n",
    "    20: 'p20',\n",
    "}\n",
    "unix_offset = {\n",
    "      1: 0.0,\n",
    "      2: 0.0,\n",
    "      4: 0.0,\n",
    "      5: 0.0,\n",
    "      6: 0.0,\n",
    "      7: 0.0,\n",
    "      8: 0.0,\n",
    "      9: 0.0,\n",
    "      10: 0.0,\n",
    "      11: 0.0,\n",
    "      12: 0.0,\n",
    "      13: 0.0,\n",
    "      14: 0.0,\n",
    "      15: 0.0,\n",
    "      16: 0.0,\n",
    "      17: 0.0,\n",
    "      18: 0.0,\n",
    "      19: 0.0,\n",
    "      20: 0.0,\n",
    "}\n",
    "\n",
    "for pid in participant_ids:\n",
    "    with open(f\"./info_players/{json_names[pid]}.json\") as file:\n",
    "            meta_info = json.load(file)\n",
    "    start_timestamp_diff = meta_info[\"start_time_system_s\"] - meta_info[\"start_time_synced_s\"]\n",
    "    unix_offset[pid] = start_timestamp_diff\n",
    "\n",
    "unix_offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conversion Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing 'all_pupil_positions'...\n",
      "Exporting all_pupil_positions in ./aggregated_data_unix\\all_pupil_positions.csv\n",
      "Exported all_pupil_positions\n"
     ]
    }
   ],
   "source": [
    "#  Run\n",
    "dfs_info = {\n",
    "    \"all_blinks\": [\"start_timestamp\", \"end_timestamp\"],\n",
    "    \"all_fixations\": [\"start_timestamp\"],\n",
    "    \"all_gaze_positions\": [\"gaze_timestamp\"],\n",
    "    \"all_pupil_positions\": [\"pupil_timestamp\"],\n",
    "    \"all_world_timestamps\": [\"# timestamps [seconds]\"],\n",
    "    \"all_fixations_on_surface_HiDrive_Studie2\": [\"world_timestamp\", \"start_timestamp\"],\n",
    "    \"all_gaze_positions_on_surface_HiDrive_Studie2\": [\"world_timestamp\", \"gaze_timestamp\"],\n",
    "    \"all_surf_positions_HiDrive_Studie2\": [\"world_timestamp\"],\n",
    "    \"all_surface_events\": [\"world_timestamp\"]\n",
    "}\n",
    "\n",
    "output_dir = \"./aggregated_data_unix\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "for file_name, timestamp_columns in dfs_info.items():\n",
    "    print(f\"Computing '{file_name}'...\")\n",
    "    ddf = data[file_name]  # Keep as Dask DataFrame\n",
    "    participant_ddfs = []\n",
    "    \n",
    "    for pid in participant_ids:\n",
    "        participant_offset = unix_offset[pid]\n",
    "        ddf_participant = ddf[ddf['participant_id'] == pid]\n",
    "        \n",
    "        for col in timestamp_columns:\n",
    "            unix_col_name = f\"{col}_unix\"\n",
    "            datetime_col_name = f\"{col}_dt\"\n",
    "            \n",
    "            # Calculate the Unix timestamp and datetime\n",
    "            ddf_participant[unix_col_name] = ddf_participant[col] + participant_offset\n",
    "            if file_name == \"all_surf_positions_HiDrive_Studie2\":\n",
    "                ddf_participant[datetime_col_name] = pd.to_datetime(ddf_participant[unix_col_name], unit='s')\n",
    "            else:\n",
    "                ddf_participant[datetime_col_name] = dd.to_datetime(ddf_participant[unix_col_name], unit='s')\n",
    "            ddf_participant[datetime_col_name] = ddf_participant[datetime_col_name].dt.tz_localize('UTC')\n",
    "            ddf_participant[datetime_col_name] = ddf_participant[datetime_col_name].dt.tz_convert('Europe/Berlin')\n",
    "\n",
    "        # Collect the processed dataframe for this participant\n",
    "        participant_ddfs.append(ddf_participant)\n",
    "    \n",
    "    # Concatenate and export results\n",
    "    output_file_path = os.path.join(output_dir, f\"{file_name}.csv\")\n",
    "    print(f\"Exporting {file_name} in {output_file_path}\")\n",
    "    if file_name == \"all_surf_positions_HiDrive_Studie2\":\n",
    "        result_df = pd.concat(participant_ddfs)\n",
    "        result_df.to_csv(output_file_path, index=False)\n",
    "    else:\n",
    "        result_ddf = dd.concat(participant_ddfs)\n",
    "        result_df.to_csv(output_file_path, single_file=True)\n",
    "\n",
    "    print(f\"Exported {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
